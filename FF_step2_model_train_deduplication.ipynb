{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc, log_loss, f1_score, accuracy_score, recall_score, precision_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pyarrow.parquet as pq\n",
    "import oss2\n",
    "import io\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据读取（已存储到相应路径）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'z_data_new2/data_merged101'\n",
    "train_data = pd.read_pickle(os.path.join(data_path,'22_05_merged_101.pkl'))\n",
    "test_data = pd.read_pickle(os.path.join(data_path,'data_raw_20250106_py_merged_91.pkl'))\n",
    "\n",
    "#去除LLM ICI columns\n",
    "# LLM\n",
    "# group_1 = ['category_first', 'category_second', 'audience', 'scenario']\n",
    "\n",
    "# ICI\n",
    "# group_2 = ['click_sum', 'buy_sum', 'convert_rate', 'ici_push_total', 'gmv_sum', 'commission_sum']\n",
    "\n",
    "# 删除LLM\n",
    "# train_data = train_data.drop(columns=group_1, errors='ignore')\n",
    "# test_data = test_data.drop(columns=group_1, errors='ignore')\n",
    "\n",
    "# 删除ICI\n",
    "# train_data = train_data.drop(columns=group_2, errors='ignore')\n",
    "# test_data = test_data.drop(columns=group_2, errors='ignore')\n",
    "\n",
    "\n",
    "encoder_path = 'z_model/encoder_data'\n",
    "model_path = 'z_model/model_data'\n",
    "print(encoder_path)\n",
    "print(model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ICI 是否选取要调整此处"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop_column = ['click_sum', 'buy_sum', 'ici_push_total', 'gmv_sum', 'commission_sum']\n",
    "\n",
    "# train_data.drop(columns=drop_column,inplace=True)\n",
    "# test_data.drop(columns=drop_column,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 正负例是否变化要调整此处"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data[(test_data['lable_x'] != 0) | (test_data['weekly_order_cnt'] < 4)]\n",
    "train_data = train_data[(train_data['lable_x'] != 0) | (train_data['weekly_order_cnt'] < 4)]\n",
    "\n",
    "print(f\"调整后的测试数据集大小: {test_data.shape}\")\n",
    "print(f\"调整后的训练数据集大小: {train_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_negative_one_with_unknown(data, columns):\n",
    "    for column in columns:\n",
    "        if column in data.columns:\n",
    "            data[column] = data[column].replace(-1, 'Unknown')\n",
    "    return data\n",
    "\n",
    "\n",
    "# 要处理的列\n",
    "columns_to_process = ['shop_title', 'brand_name', 'audience_1', 'audience_2','brand_real_1','brand_real_2','scenario_1','scenario_2','feature1','feature2','feature3','feature4','feature5','feature6']\n",
    "\n",
    "\n",
    "train_data = replace_negative_one_with_unknown(train_data, columns_to_process)\n",
    "test_data = replace_negative_one_with_unknown(test_data, columns_to_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = train_data.columns\n",
    "list2 = test_data.columns\n",
    "unique_to_list1 = set(list1) - set(list2)\n",
    "unique_to_list2 = set(list2) - set(list1)\n",
    "unique_to_list1 = list(unique_to_list1)\n",
    "unique_to_list2 = list(unique_to_list2)\n",
    "\n",
    "print(\"在 train_data 中但不在 test_data 中的列名:\", unique_to_list1)\n",
    "print(\"在 test_data 中但不在 train_data 中的列名:\", unique_to_list2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对原始数据整数编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = True#fit 为 True，则训练新的编码器并保存；如果 fit 为 False，则从存储中加载已保存的编码器\n",
    "# fit = False\n",
    "if fit:\n",
    "    label_encoder_level_one_category_name = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "    label_encoder_category_name = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "    label_encoder_category_category_first = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "    label_encoder_category_category_second = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "    label_encoder_category_audience = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "    # label_encoder_category_scenario = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "    label_encoder_brand_name = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "    label_encoder_shop_title = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "    label_encoder_feature1 = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "    label_encoder_feature2 = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "    label_encoder_feature3 = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "    label_encoder_feature4 = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "    label_encoder_feature5 = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "    label_encoder_feature6 = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "    label_encoder_scenario_1 = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "    label_encoder_scenario_2 = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "    label_encoder_brand_real_1 = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "    label_encoder_brand_real_2 = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "    label_encoder_audience_1 = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "    label_encoder_audience_2 = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "\n",
    "\n",
    "\n",
    "    # label_encoder_p = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "\n",
    "    label_encoder_level_one_category_name.fit(train_data[\"level_one_category_name\"].value_counts().reset_index().dropna()[\"level_one_category_name\"].to_numpy().reshape(-1,1))\n",
    "    label_encoder_category_name.fit(train_data[\"category_name\"].value_counts().reset_index().dropna()[\"category_name\"].to_numpy().reshape(-1,1))\n",
    "    label_encoder_category_category_first.fit(train_data[\"category_first\"].value_counts().reset_index().dropna()[\"category_first\"].to_numpy().reshape(-1,1))\n",
    "    label_encoder_category_category_second.fit(train_data[\"category_second\"].value_counts().reset_index().dropna()[\"category_second\"].to_numpy().reshape(-1,1))\n",
    "    # label_encoder_category_audience.fit(train_data[\"audience\"].value_counts().reset_index().dropna()[\"audience\"].to_numpy().reshape(-1,1))\n",
    "    # label_encoder_category_scenario.fit(train_data[\"scenario\"].value_counts().reset_index().dropna()[\"scenario\"].to_numpy().reshape(-1,1))\n",
    "    label_encoder_brand_name.fit(train_data[\"brand_name\"].value_counts().reset_index().dropna()[\"brand_name\"].to_numpy().reshape(-1,1))\n",
    "    label_encoder_shop_title.fit(train_data[\"shop_title\"].value_counts().reset_index().dropna()[\"shop_title\"].to_numpy().reshape(-1,1))\n",
    "    label_encoder_feature1.fit(train_data[\"feature1\"].value_counts().reset_index().dropna()[\"feature1\"].to_numpy().reshape(-1,1))\n",
    "    label_encoder_feature2.fit(train_data[\"feature2\"].value_counts().reset_index().dropna()[\"feature2\"].to_numpy().reshape(-1,1))\n",
    "    label_encoder_feature3.fit(train_data[\"feature3\"].value_counts().reset_index().dropna()[\"feature3\"].to_numpy().reshape(-1,1))\n",
    "    label_encoder_feature4.fit(train_data[\"feature4\"].value_counts().reset_index().dropna()[\"feature4\"].to_numpy().reshape(-1,1))\n",
    "    label_encoder_feature5.fit(train_data[\"feature5\"].value_counts().reset_index().dropna()[\"feature5\"].to_numpy().reshape(-1,1))\n",
    "    label_encoder_feature6.fit(train_data[\"feature6\"].value_counts().reset_index().dropna()[\"feature6\"].to_numpy().reshape(-1,1))\n",
    "    label_encoder_scenario_1.fit(train_data[\"scenario_1\"].value_counts().reset_index().dropna()[\"scenario_1\"].to_numpy().reshape(-1,1))\n",
    "    label_encoder_scenario_2.fit(train_data[\"scenario_2\"].value_counts().reset_index().dropna()[\"scenario_2\"].to_numpy().reshape(-1,1))\n",
    "    label_encoder_brand_real_1.fit(train_data[\"brand_real_1\"].value_counts().reset_index().dropna()[\"brand_real_1\"].to_numpy().reshape(-1,1))\n",
    "    label_encoder_brand_real_2.fit(train_data[\"brand_real_2\"].value_counts().reset_index().dropna()[\"brand_real_2\"].to_numpy().reshape(-1,1))\n",
    "    label_encoder_audience_1.fit(train_data[\"audience_1\"].value_counts().reset_index().dropna()[\"audience_1\"].to_numpy().reshape(-1,1))\n",
    "    label_encoder_audience_2.fit(train_data[\"audience_2\"].value_counts().reset_index().dropna()[\"audience_2\"].to_numpy().reshape(-1,1))\n",
    "\n",
    "\n",
    "    # label_encoder_p.fit(train_data.dropna(subset=[\"audience\"])[\"audience\"].to_numpy().reshape(-1,1))\n",
    "    pickle.dump(label_encoder_level_one_category_name, open(os.path.join(encoder_path, \"22_28_label_encoder_level_one_category_name.pkl\"), \"wb\"))\n",
    "    pickle.dump(label_encoder_category_name, open(os.path.join(encoder_path, \"22_28_label_encoder_category_name.pkl\"), \"wb\"))\n",
    "    pickle.dump(label_encoder_category_category_first, open(os.path.join(encoder_path, \"22_28_label_encoder_category_first.pkl\"), \"wb\"))\n",
    "    pickle.dump(label_encoder_category_category_second, open(os.path.join(encoder_path, \"22_28_label_encoder_category_second.pkl\"), \"wb\"))\n",
    "    pickle.dump(label_encoder_category_audience, open(os.path.join(encoder_path, \"22_28_label_encoder_audience.pkl\"), \"wb\"))\n",
    "    # pickle.dump(label_encoder_category_scenario, open(os.path.join(encoder_path, \"22_28_label_encoder_scenario.pkl\"), \"wb\"))\n",
    "    # pickle.dump(label_encoder_p, open(os.path.join(encoder_path, \"label_encoder2_p.pkl/\"), \"wb\"))\n",
    "    pickle.dump(label_encoder_brand_name, open(os.path.join(encoder_path, \"22_28_label_encoder_brand_name.pkl\"), \"wb\"))\n",
    "    pickle.dump(label_encoder_shop_title, open(os.path.join(encoder_path, \"22_28_llabel_encoder_shop_title.pkl\"), \"wb\"))\n",
    "    pickle.dump(label_encoder_feature1, open(os.path.join(encoder_path, \"22_28_label_encoder_feature1.pkl\"), \"wb\"))\n",
    "    pickle.dump(label_encoder_feature2, open(os.path.join(encoder_path, \"22_28_label_encoder_feature2.pkl\"), \"wb\"))\n",
    "    pickle.dump(label_encoder_feature3, open(os.path.join(encoder_path, \"22_28_label_encoder_feature3.pkl\"), \"wb\"))\n",
    "    pickle.dump(label_encoder_feature4, open(os.path.join(encoder_path, \"22_28_label_encoder_feature4.pkl\"), \"wb\"))\n",
    "    pickle.dump(label_encoder_feature5, open(os.path.join(encoder_path, \"22_28_label_encoder_feature5.pkl\"), \"wb\"))\n",
    "    pickle.dump(label_encoder_feature6, open(os.path.join(encoder_path, \"22_28_label_encoder_feature6.pkl\"), \"wb\"))\n",
    "    pickle.dump(label_encoder_scenario_1, open(os.path.join(encoder_path, \"22_28_label_encoder_scenario_1.pkl\"), \"wb\"))\n",
    "    pickle.dump(label_encoder_scenario_2, open(os.path.join(encoder_path, \"22_28_label_encoder_scenario_2.pkl\"), \"wb\"))\n",
    "    pickle.dump(label_encoder_brand_real_1, open(os.path.join(encoder_path, \"22_28_label_encoder_brand_real_1.pkl\"), \"wb\"))\n",
    "    pickle.dump(label_encoder_brand_real_2, open(os.path.join(encoder_path, \"22_28_label_encoder_brand_real_2.pkl\"), \"wb\"))   \n",
    "    pickle.dump(label_encoder_audience_1, open(os.path.join(encoder_path, \"22_28_label_encoder_audience_1.pkl\"), \"wb\"))\n",
    "    pickle.dump(label_encoder_audience_2, open(os.path.join(encoder_path, \"22_28_label_encoder_audience_2.pkl\"), \"wb\"))  \n",
    "    print(\"1\")\n",
    "else:\n",
    "    label_encoder_level_one_category_name = pickle.load(open(os.path.join(encoder_path, \"22_28_label_encoder_level_one_category_name.pkl\"), \"rb\"))\n",
    "    label_encoder_category_name = pickle.load(open(os.path.join(encoder_path, \"22_28_label_encoder_category_name.pkl\"), \"rb\"))\n",
    "    label_encoder_category_category_first = pickle.load(open(os.path.join(encoder_path, \"22_28_label_encoder_category_first.pkl\"), \"rb\"))\n",
    "    label_encoder_category_category_second = pickle.load(open(os.path.join(encoder_path, \"22_28_label_encoder_category_second.pkl\"), \"rb\"))\n",
    "    label_encoder_category_audience = pickle.load(open(os.path.join(encoder_path, \"22_28_label_encoder_audience.pkl\"), \"rb\"))\n",
    "    label_encoder_category_scenario = pickle.load(open(os.path.join(encoder_path, \"22_28_label_encoder_scenario.pkl\"), \"rb\"))\n",
    "    # label_encoder_p = pickle.load(open(os.path.join(encoder_path, \"label_encoder2_p.pkl\"), \"rb\"))\n",
    "    print(\"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#打印出每一列的列名以及该列中不同值的数量\n",
    "columns = train_data.columns\n",
    "for col in columns:\n",
    "    print(f\"列名: {col}, 不同值的数量: {train_data[col].nunique()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在进行转换前，先处理缺失值\n",
    "train_data[\"level_one_category_name\"] = train_data[\"level_one_category_name\"].fillna('Unknown')  # 用 'Unknown' 填充 NaN 值\n",
    "train_data[\"category_name\"] = train_data[\"category_name\"].fillna('Unknown')  # 用 'Unknown' 填充 NaN 值\n",
    "train_data[\"category_first\"] = train_data[\"category_first\"].fillna('Unknown')  # 用 'Unknown' 填充 NaN 值\n",
    "train_data[\"category_second\"] = train_data[\"category_second\"].fillna('Unknown')  # 用 'Unknown' 填充 NaN 值\n",
    "# train_data[\"audience\"] = train_data[\"audience\"].fillna('Unknown')  # 用 'Unknown' 填充 NaN 值\n",
    "# train_data[\"scenario\"] = train_data[\"scenario\"].fillna('Unknown')  # 用 'Unknown' 填充 NaN 值\n",
    "\n",
    "\n",
    "test_data[\"level_one_category_name\"] = test_data[\"level_one_category_name\"].fillna('Unknown')\n",
    "test_data[\"category_name\"] = test_data[\"category_name\"].fillna('Unknown')\n",
    "test_data[\"category_first\"] = test_data[\"category_first\"].fillna('Unknown')\n",
    "test_data[\"category_second\"] = test_data[\"category_second\"].fillna('Unknown')\n",
    "# test_data[\"audience\"] = test_data[\"audience\"].fillna('Unknown')\n",
    "# test_data[\"scenario\"] = test_data[\"scenario\"].fillna('Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(train_data[\"level_one_category_name\"] == 'Unknown'))\n",
    "print(sum(train_data[\"category_name\"] == 'Unknown'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对 train_data 进行编码处理\n",
    "train_data[\"level_one_category_name\"] = label_encoder_level_one_category_name.transform(train_data[\"level_one_category_name\"].to_numpy().reshape(-1, 1))\n",
    "train_data[\"category_name\"] = label_encoder_category_name.transform(train_data[\"category_name\"].to_numpy().reshape(-1, 1))\n",
    "train_data[\"category_first\"] = label_encoder_level_one_category_name.transform(train_data[\"category_first\"].to_numpy().reshape(-1, 1))\n",
    "train_data[\"category_second\"] = label_encoder_category_name.transform(train_data[\"category_second\"].to_numpy().reshape(-1, 1))\n",
    "# train_data[\"audience\"] = label_encoder_level_one_category_name.transform(train_data[\"audience\"].to_numpy().reshape(-1, 1))\n",
    "# train_data[\"scenario\"] = label_encoder_category_name.transform(train_data[\"scenario\"].to_numpy().reshape(-1, 1))\n",
    "train_data[\"brand_name\"] = label_encoder_brand_name.transform(train_data[\"brand_name\"].to_numpy().reshape(-1, 1))\n",
    "train_data[\"shop_title\"] = label_encoder_shop_title.transform(train_data[\"shop_title\"].to_numpy().reshape(-1, 1))\n",
    "train_data[\"feature1\"] = label_encoder_feature1.transform(train_data[\"feature1\"].to_numpy().reshape(-1, 1))\n",
    "train_data[\"feature2\"] = label_encoder_feature2.transform(train_data[\"feature2\"].to_numpy().reshape(-1, 1))\n",
    "train_data[\"feature3\"] = label_encoder_feature3.transform(train_data[\"feature3\"].to_numpy().reshape(-1, 1))\n",
    "train_data[\"feature4\"] = label_encoder_feature4.transform(train_data[\"feature4\"].to_numpy().reshape(-1, 1))\n",
    "train_data[\"feature5\"] = label_encoder_feature5.transform(train_data[\"feature5\"].to_numpy().reshape(-1, 1))\n",
    "train_data[\"feature6\"] = label_encoder_feature6.transform(train_data[\"feature6\"].to_numpy().reshape(-1, 1))\n",
    "train_data[\"scenario_1\"] = label_encoder_scenario_1.transform(train_data[\"scenario_1\"].to_numpy().reshape(-1, 1))\n",
    "train_data[\"scenario_2\"] = label_encoder_scenario_2.transform(train_data[\"scenario_2\"].to_numpy().reshape(-1, 1))\n",
    "train_data[\"brand_real_1\"] = label_encoder_brand_real_1.transform(train_data[\"brand_real_1\"].to_numpy().reshape(-1, 1))\n",
    "train_data[\"brand_real_2\"] = label_encoder_brand_real_2.transform(train_data[\"brand_real_2\"].to_numpy().reshape(-1, 1))\n",
    "train_data[\"audience_1\"] = label_encoder_audience_1.transform(train_data[\"audience_1\"].to_numpy().reshape(-1, 1))\n",
    "train_data[\"audience_2\"] = label_encoder_audience_2.transform(train_data[\"audience_2\"].to_numpy().reshape(-1, 1))\n",
    "\n",
    "# label_encoder_feature6.fit(train_data[\"feature6\"].value_counts().reset_index().dropna()[\"feature6\"].to_numpy().reshape(-1,1))\n",
    "# label_encoder_scenario_1.fit(train_data[\"scenario_1\"].value_counts().reset_index().dropna()[\"scenario_1\"].to_numpy().reshape(-1,1))\n",
    "# label_encoder_scenario_2.fit(train_data[\"scenario_2\"].value_counts().reset_index().dropna()[\"scenario_2\"].to_numpy().reshape(-1,1))\n",
    "# label_encoder_brand_real_1.fit(train_data[\"brand_real_1\"].value_counts().reset_index().dropna()[\"brand_real_1\"].to_numpy().reshape(-1,1))\n",
    "# label_encoder_brand_real_2.fit(train_data[\"brand_real_2\"].value_counts().reset_index().dropna()[\"brand_real_2\"].to_numpy().reshape(-1,1))\n",
    "# label_encoder_audience_1.fit(train_data[\"audience_1\"].value_counts().reset_index().dropna()[\"audience_1\"].to_numpy().reshape(-1,1))\n",
    "# label_encoder_audience_2.fit(train_data[\"audience_2\"].value_counts().reset_index().dropna()[\"audience_2\"].to_numpy().reshape(-1,1))\n",
    "\n",
    "\n",
    "# 定义训练目标\n",
    "target = train_data['lable_x']\n",
    "\n",
    "# 打印训练数据正负例分布\n",
    "print(f\"全部的训练数据正负例分布: {train_data['lable_x'].value_counts()}\")\n",
    "\n",
    "# 不再进行划分，直接使用全部的 train_data 进行训练\n",
    "X_train = train_data.drop(columns=['lable_x'])\n",
    "y_train = target\n",
    "\n",
    "# 对 test_data 进行编码处理\n",
    "test_data[\"level_one_category_name\"] = label_encoder_level_one_category_name.transform(test_data[\"level_one_category_name\"].to_numpy().reshape(-1, 1))\n",
    "test_data[\"category_name\"] = label_encoder_category_name.transform(test_data[\"category_name\"].to_numpy().reshape(-1, 1))\n",
    "test_data[\"category_first\"] = label_encoder_level_one_category_name.transform(test_data[\"category_first\"].to_numpy().reshape(-1, 1))\n",
    "test_data[\"category_second\"] = label_encoder_category_name.transform(test_data[\"category_second\"].to_numpy().reshape(-1, 1))\n",
    "# test_data[\"audience\"] = label_encoder_level_one_category_name.transform(test_data[\"audience\"].to_numpy().reshape(-1, 1))\n",
    "# test_data[\"scenario\"] = label_encoder_category_name.transform(test_data[\"scenario\"].to_numpy().reshape(-1, 1))\n",
    "test_data[\"brand_name\"] = label_encoder_brand_name.transform(test_data[\"brand_name\"].to_numpy().reshape(-1, 1))\n",
    "test_data[\"shop_title\"] = label_encoder_shop_title.transform(test_data[\"shop_title\"].to_numpy().reshape(-1, 1))\n",
    "test_data[\"feature1\"] = label_encoder_feature1.transform(test_data[\"feature1\"].to_numpy().reshape(-1, 1))\n",
    "test_data[\"feature2\"] = label_encoder_feature2.transform(test_data[\"feature2\"].to_numpy().reshape(-1, 1))\n",
    "test_data[\"feature3\"] = label_encoder_feature3.transform(test_data[\"feature3\"].to_numpy().reshape(-1, 1))\n",
    "test_data[\"feature4\"] = label_encoder_feature4.transform(test_data[\"feature4\"].to_numpy().reshape(-1, 1))\n",
    "test_data[\"feature5\"] = label_encoder_feature5.transform(test_data[\"feature5\"].to_numpy().reshape(-1, 1))\n",
    "test_data[\"feature6\"] = label_encoder_feature6.transform(test_data[\"feature6\"].to_numpy().reshape(-1, 1))\n",
    "test_data[\"scenario_1\"] = label_encoder_scenario_1.transform(test_data[\"scenario_1\"].to_numpy().reshape(-1, 1))\n",
    "test_data[\"scenario_2\"] = label_encoder_scenario_2.transform(test_data[\"scenario_2\"].to_numpy().reshape(-1, 1))\n",
    "test_data[\"brand_real_1\"] = label_encoder_brand_real_1.transform(test_data[\"brand_real_1\"].to_numpy().reshape(-1, 1))\n",
    "test_data[\"brand_real_2\"] = label_encoder_brand_real_2.transform(test_data[\"brand_real_2\"].to_numpy().reshape(-1, 1))\n",
    "test_data[\"audience_1\"] = label_encoder_audience_1.transform(test_data[\"audience_1\"].to_numpy().reshape(-1, 1))\n",
    "test_data[\"audience_2\"] = label_encoder_audience_2.transform(test_data[\"audience_2\"].to_numpy().reshape(-1, 1))\n",
    "################\n",
    "\n",
    "\n",
    "# 打印测试数据正负例分布\n",
    "print(f\"全部的额外测试数据正负例分布: {test_data['lable_x'].value_counts()}\")\n",
    "\n",
    "# 不再对 test_data 进行划分，直接作为测试集\n",
    "X_test = test_data.drop(columns=['lable_x'])\n",
    "y_test = test_data['lable_x']\n",
    "\n",
    "# 打印各个数据集的正负例分布\n",
    "print(f\"训练数据正负例分布: {y_train.value_counts()}\")\n",
    "print(f\"测试数据正负例分布: {y_test.value_counts()}\")\n",
    "\n",
    "# 输出训练集和测试集的大小\n",
    "print(f\"训练集大小: {X_train.shape}\")\n",
    "print(f\"测试集大小: {X_test.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_drop_columns = ['title_x','pricejson_update_time','lable_x','click_num_x','buy_num_x','feature','obj_scenario','time_scenario','dep_scenario','subj_scenario','brand_quality','brand_real','category',\n",
    "                    'title','item_id','ranked_categories.p_diffjson','ranked_categories.discountjson','ranked_categories.pricejson','ranked_categories.annual_voljson','item_title','ici_card','ici'] \n",
    "tmp_drop_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_temp = X_train.drop(columns=[x for x in tmp_drop_columns if x in X_train.columns])\n",
    "\n",
    "\n",
    "\n",
    "# print(shape(X_train))\n",
    "\n",
    "print(x_temp.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "params = {\n",
    "    \"task\": \"train\",\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"num_class\": 1,\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": \"auc\",\n",
    "    \"num_leaves\": 31,\n",
    "    \"max_depth\": 10,\n",
    "    \"boost_from_average\": True,\n",
    "    \"num_threads\": 30,\n",
    "    \"feature_fraction\": 0.8,\n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"min_data_in_leaf\": 100,\n",
    "    \"learning_rate\": 0.02,\n",
    "    \"max_bin\": 5000,\n",
    "    \"is_unbalance\": True,\n",
    "    'lambda_l1': 0.1, \n",
    "    'lambda_l2': 0.1, \n",
    "}\n",
    "\n",
    "\n",
    "current_time = datetime.now().strftime(\"%Y%m%d_%H:%M\")\n",
    "X_train_resampled, y_train_resampled = X_train, y_train\n",
    "lgb_train = lgb.Dataset(X_train_resampled.drop(columns=[x for x in tmp_drop_columns if x in X_train_resampled.columns]), y_train_resampled, params=params)\n",
    "X_test_resampled, y_test_resampled = X_test, y_test\n",
    "lgb_test = lgb.Dataset(X_test_resampled.drop(columns=[x for x in tmp_drop_columns if x in X_test_resampled.columns]), y_test_resampled, reference=lgb_train)\n",
    "\n",
    "# 开始训练模型\n",
    "evals_result = {}\n",
    "lgb_model = lgb.train(params,\n",
    "                      lgb_train,\n",
    "                      num_boost_round=2000,\n",
    "                      valid_sets=lgb_test,  # 这里使用的是 test 数据集来进行模型评估\n",
    "                      callbacks=[lgb.early_stopping(100), lgb.record_evaluation(evals_result)])\n",
    "\n",
    "\n",
    "lgb.plot_metric(booster=evals_result, metric='auc')\n",
    "lgb.plot_importance(lgb_model, max_num_features=20)\n",
    "current_time = datetime.now().strftime(\"%Y%m%d_%H:%M\")\n",
    "pickle.dump(lgb_model, open(os.path.join(model_path, f\"22_28_model_{current_time}.pkl\"), \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see = X_val_resampled.drop(columns=[x for x in tmp_drop_columns if x in X_val_resampled.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tree_index = 1  \n",
    "# lgb.plot_tree(lgb_model, tree_index=tree_index,figsize=(40, 20))\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = lgb_model.feature_importance()\n",
    "feature_names = lgb_model.feature_name()\n",
    "\n",
    "# 打印每个特征的重要性\n",
    "feature_importance = []\n",
    "for feature, score in zip(feature_names, importance):\n",
    "    feature_importance.append({\"feature\": feature, \"score\": score})\n",
    "feature_importance = pd.DataFrame(feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_feature_importance = feature_importance.sort_values(by='score', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_preds = lgb_model.predict(X_test.drop(columns=tmp_drop_columns))\n",
    "test_preds = lgb_model.predict(X_test.drop(columns=tmp_drop_columns, errors='ignore'))\n",
    "\n",
    "test_res = (test_preds > 0.5).astype(int)\n",
    "\n",
    "aucs = roc_auc_score(y_test.values, test_preds)\n",
    "# logloss = log_loss(y_test.values, test_preds, eps=1e-12)\n",
    "logloss = log_loss(y_test.values, test_preds)\n",
    "\n",
    "precision = precision_score(y_test.values, test_res)\n",
    "accuracy = accuracy_score(y_test.values, test_res)\n",
    "recall = recall_score(y_test.values, test_res)\n",
    "f1 = f1_score(y_test.values, test_res)\n",
    "\n",
    "print({\"auc\": aucs, \"logloss\": logloss})\n",
    "print({\"precision\": precision, \"recall\": recall, \"f1\": f1, \"accuracy\": accuracy})\n",
    "\n",
    "# display(test_x.head())\n",
    "# display(train_x.head())\n",
    "\n",
    "pos = np.sum(y_test)\n",
    "print(len(y_test),pos,pos/len(y_test),len(y_test)-pos)\n",
    "pos = np.sum(y_train)\n",
    "print(len(y_train),pos,pos/len(y_train),len(y_train)-pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for thred in [0.05, 0.1,0.2,0.25,0.3, 0.35,0.4,0.45, 0.5,0.55, 0.6, 0.65, 0.7,0.75, 0.8, 0.85, 0.9, 0.95]:\n",
    "    print(f\"==============================thred={thred}==================================\")\n",
    "    print(classification_report(y_test, (test_preds > thred).astype(int), target_names=['负例', '正例'], digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将 predictions 和 target 合并成一个 DataFrame\n",
    "P_T = pd.DataFrame({\n",
    "    'Predictions': test_preds,\n",
    "    'Target': target\n",
    "})\n",
    "\n",
    "\n",
    "TT = P_T[P_T['Target' ]== 1]\n",
    "TF = P_T[P_T['Target' ]== 0]\n",
    "\n",
    "TT = TT['Predictions']\n",
    "TF = TF['Predictions']\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.kdeplot(TT, shade=True, color=\"b\", label='TT')\n",
    "sns.kdeplot(TF, shade=True, color=\"r\", label='TF')\n",
    "plt.title(\"Predictions vs. Target Probability Density\", fontsize=16)\n",
    "plt.xlabel(\"Probability / Label\", fontsize=12)\n",
    "plt.ylabel(\"Density\", fontsize=12)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lichen_special",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
